# E-commerce Data Pipeline & Analytics Dashboard üöÄüìä

[![GitHub stars](https://img.shields.io/github/stars/abrahamkoloboe27/Setup-Databases-With-Docker?style=social)](https://github.com/abrahamkoloboe27/Setup-Databases-With-Docker)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/Docker-2496ED?logo=docker&logoColor=white)](https://www.docker.com)



## Aper√ßu du Projet

Ce projet propose une solution compl√®te de traitement de donn√©es pour une plateforme e-commerce, couvrant la **g√©n√©ration de donn√©es synth√©tiques**, le **pipeline de traitement ELT/ETL**, le stockage dans un **Data Lake/ Warehouse** et la visualisation via **Apache Superset**.




## Architecture Technique üèóÔ∏è

### Sch√©ma d'architecture
*Ins√©rez ici un diagramme ou une capture illustrant l‚Äôarchitecture globale (Data Lake, Data Warehouse, Airflow, Superset, etc.).*

![Diagramme d'architecture](./docs/images/architecture_diagram.png)



## Technologies Utilis√©es

| **Composant**               | **Technologie**                                                     | **Ic√¥ne/Image**                           |
|-----------------------------|----------------------------------------------------------------------|-------------------------------------------|
| **G√©n√©ration de Donn√©es**   | Python, Faker                                                        | ![Python](https://img.shields.io/badge/Python-3776AB?logo=python&logoColor=white) |
| **Orchestration**           | Apache Airflow                                                       | ![Airflow](https://img.shields.io/badge/Airflow-017CEE?logo=apache-airflow&logoColor=white) |
| **Traitement**              | Polars                                                               | ![Polars](https://img.shields.io/badge/Polars-2A2A2A?logo=python&logoColor=white)  |
| **Stockage (Data Lake)**    | Minio (S3-compatible)                                                | ![Minio](https://img.shields.io/badge/Minio-00ADEF?logo=minio&logoColor=white) |
| **Data Warehouse**          | PostgreSQL                                                           | ![PostgreSQL](https://img.shields.io/badge/PostgreSQL-336791?logo=postgresql&logoColor=white) |
| **Visualisation**           | Apache Superset                                                      | ![Superset](https://img.shields.io/badge/Superset-EC6A37?logo=apache-superset&logoColor=white) |
| **Monitoring**              | Grafana, Prometheus (pr√©vu)                                            | ![Grafana](https://img.shields.io/badge/Grafana-F46800?logo=grafana&logoColor=white)  |



## Workflow du Pipeline üîÑ

Le pipeline s'organise en 4 √©tapes principales :

| **√âtape**               | **Description**                                                                                                             | **Technologies / Outils**                                      |
|-------------------------|-----------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|
| **Extraction**          | R√©cup√©ration des donn√©es depuis PostgreSQL vers le Data Lake (Minio ‚Äì couche Bronze).                                        | PostgreSQL, Minio                                              |
| **Transformation**      | Nettoyage, filtrage, agr√©gations et calcul des KPI. Les donn√©es passent par une couche Silver (nettoy√©es) puis Gold (agr√©g√©es). | Polars                                                         |
| **Chargement**          | Insertion des donn√©es transform√©es dans le Data Warehouse en utilisant un mod√®le en √©toile (Star Schema).                     | PostgreSQL, Vues mat√©rialis√©es                                   |
| **Visualisation**       | Cr√©ation d‚Äôun dashboard interactif pour suivre les performances (CA, produits, clients, etc.).                              | Apache Superset                                                |

*Astuce : Ajoutez une capture du dashboard Superset pour une meilleure illustration.*

![Dashboard Superset](./docs/images/superset_dashboard.png)



## Focus sur Apache Airflow üêç‚è∞

Le dossier `dags/` contient la d√©finition des workflows Airflow. En particulier, le fichier `dags/ecommerce_pipeline.py` orchestre l‚Äôensemble du pipeline :

- **D√©finition du DAG**  
  Le DAG est configur√© pour s‚Äôex√©cuter √† une fr√©quence d√©finie (ex : quotidiennement) et comprend plusieurs t√¢ches :
  - **Extraction** : R√©cup√©rer les donn√©es depuis PostgreSQL.
  - **Transformation** : Utiliser Polars pour nettoyer et transformer les donn√©es.
  - **Chargement** : Ins√©rer les donn√©es dans le Data Warehouse.
  
- **Graphique du DAG**  
  *Ins√©rez ici une capture du graphique de t√¢ches d‚ÄôAirflow pour visualiser l‚Äôordonnancement du pipeline.*

![Graphique du DAG Airflow](./docs/images/airflow_dag.png)



## D√©marrage Rapide üöÄ

### Pr√©requis

- **Docker** & **Docker-compose**
- **Python 3.10+**

### Installation

1. **Cloner le d√©p√¥t**
   ```bash
   git clone https://github.com/abrahamkoloboe27/E-Commerce-Data-Pipeline-And-Dashboard-With-Apache-Superset.git
   cd E-Commerce-Data-Pipeline-And-Dashboard-With-Apache-Superset
   ```

2. **D√©marrer l'infrastructure**
   ```bash
   docker-compose up -d
   ```

3. **Configurer Airflow**
   ```bash
   airflow db init
   airflow users create --username admin --password admin --role Admin --email admin@example.com
   ```

4. **Lancer le pipeline**
   ```bash
   airflow dags unpause ecommerce_pipeline
   ```



## Monitoring & Acc√®s üñ•Ô∏è

| **Service**       | **URL**                        | **Port**     |
|-------------------|--------------------------------|--------------|
| **Airflow**       | [http://localhost:8080](http://localhost:8080)       | 8080         |
| **Minio**         | [http://localhost:9001](http://localhost:9001)       | 9001         |
| **Superset**      | [http://localhost:8088](http://localhost:8088)       | 8088         |
| **PostgreSQL**    | - Prod : `localhost:5432`      | 5432 (Prod)  |
|                   | - Analytics : `localhost:5433` | 5433         |



## Am√©liorations Futures üîÆ

- **Tests de Qualit√© des Donn√©es** : Int√©grer [Great Expectations](https://greatexpectations.io) pour automatiser la validation des donn√©es.
- **Rejeu de Donn√©es** : Mettre en place des m√©canismes permettant de rejouer le pipeline en cas d'erreur.
- **Machine Learning** : Ajouter des modules de recommandation et d'analyse pr√©dictive.
- **Monitoring Avanc√©** : Utiliser Prometheus et Grafana pour un suivi d√©taill√© des performances du pipeline.



## Ressources Compl√©mentaires

- [Guide de D√©ploiement](./docs/deployment_guide.md)
- [Specifications Techniques](./docs/technical_specs.md)
- [Mod√®le de Donn√©es](./docs/data_model.png)


## About

Ce projet a √©t√© d√©velopp√© pour d√©montrer une solution compl√®te de pipeline de donn√©es pour une plateforme e-commerce. Pour plus d‚Äôinformations ou pour toute question, n‚Äôh√©sitez pas √† me contacter :

- **Email** : abklb27@gmail.com
- **LinkedIn** : [Abraham KOLOBOE](https://www.linkedin.com/in/abraham-zacharie-koloboe-data-science-ia-generative-llms-machine-learning/)
