# E-commerce Data Pipeline & Analytics Dashboard

## ğŸ“Š AperÃ§u du projet
Ce projet est une solution complÃ¨te de traitement de donnÃ©es pour une plateforme e-commerce, implÃ©mentant :
- GÃ©nÃ©ration de donnÃ©es synthÃ©tiques
- Pipeline de traitement ELT/ETL
- Data Lake organisÃ© (Bronze-Silver-Gold)
- Data Warehouse avec modÃ©lisation dimensionnelle
- Tableau de bord analytique

## ğŸ›  Architecture technique
![SchÃ©ma d'architecture](lien_vers_image_schÃ©ma.png) *[Optionnel : Ajouter un diagramme d'architecture]*

### Technologies clÃ©s :
- **GÃ©nÃ©ration donnÃ©es** : Python + Faker
- **Orchestration** : Apache Airflow
- **Traitement donnÃ©es** : Polars
- **Stockage** : 
  - Data Lake : Minio (S3-compatible)
  - Data Warehouse : PostgreSQL
- **Visualisation** : Apache Superset

## ğŸ”„ Workflow du pipeline
1. **Extraction des donnÃ©es**
   - Connecteur PostgreSQL â†’ Minio (Bronze)
   - Extraction incrÃ©mentale (par date) ou totale

2. **Transformation des donnÃ©es** (via Polars)
   - **Couche Silver** :
     - Nettoyage des donnÃ©es
     - Filtrage des anomalies
     - Standardisation des formats
   
   - **Couche Gold** :
     - AgrÃ©gations mÃ©tiers
     - Calculs de KPI
     - PrÃ©paration pour l'analytique

3. **Chargement dans le Data Warehouse**
   - ModÃ¨le en Ã©toile (Star Schema)
   - Tables de faits :
     - Transactions
     - Interactions utilisateurs
     - Stocks
   - Tables de dimensions :
     - Produits
     - Clients
     - Dates
     - Fournisseurs

4. **Visualisation**
   - Dashboard interactif avec Apache Superset
   - MÃ©triques principales :
     - CA par pÃ©riode
     - Performances produits
     - Comportement clients
     - Analyses gÃ©ographiques

## ğŸ“‚ Structure des donnÃ©es
### Data Lake (Minio)
```
minio/
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ raw_sales/
â”‚   â”œâ”€â”€ raw_users/
â”‚   â””â”€â”€ ... (tables brutes)
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ cleaned_sales/
â”‚   â”œâ”€â”€ enriched_users/
â”‚   â””â”€â”€ ... (donnÃ©es traitÃ©es)
â””â”€â”€ gold/
    â”œâ”€â”€ sales_kpi/
    â”œâ”€â”€ customer_lifetime_value/
    â””â”€â”€ ... (agrÃ©gats mÃ©tiers)
```

### Data Warehouse (PostgreSQL)
- SchÃ©ma `dim_*` pour les dimensions
- SchÃ©ma `fact_*` pour les faits
- Vues matÃ©rialisÃ©es pour les requÃªtes frÃ©quentes

## ğŸš€ DÃ©marrage rapide
### PrÃ©requis
- Docker + Docker-compose
- Python 3.10+

### Installation
1. Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/votre-repo/ecommerce-pipeline.git
cd ecommerce-pipeline
```

2. DÃ©marrer l'infrastructure
```bash
docker-compose up -d
```

3. Configurer Airflow
```bash
airflow db init
airflow users create --username admin --password admin --role Admin --email admin@example.com
```

4. Lancer le pipeline
```bash
airflow dags unpause ecommerce_pipeline
```

## ğŸ” Monitoring
- **Airflow** : http://localhost:8080
- **Minio** : http://localhost:9001
- **Superset** : http://localhost:8088
- **PostgreSQL** : 
  - Prod: port 5432
  - Analytics: port 5433

## ğŸ“ˆ AmÃ©liorations futures
- ImplÃ©mentation de tests de qualitÃ© des donnÃ©es (Great Expectations)
- Ajout de mÃ©canismes de rejeu de donnÃ©es
- IntÃ©gration de Machine Learning (recommandations)
- Monitoring dÃ©taillÃ© avec Prometheus/Grafana

## ğŸ“š Documentation complÃ©mentaire
- [Specifications techniques](docs/SPECS.md)
- [Guide de dÃ©ploiement](docs/DEPLOYMENT.md)
- [ModÃ¨le de donnÃ©es](docs/DATA_MODEL.md)
